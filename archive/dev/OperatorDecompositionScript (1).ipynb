{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a2a011-3291-460b-b8a8-25bb2d7f3951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS THE ORIGINAL ONE THAT IS NOW GIVING ERROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e57174d-9075-4bac-8320-94262820c5e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pytorch_env (Python 3.13.0)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/ziyao/583-pytorch-inductor/pytorch_env/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from torch._functorch.aot_autograd import aot_module_simplified\n",
    "from torch._decomp import core_aten_decompositions\n",
    "from torch.fx import GraphModule\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class MyModule(torch.nn.Module):\n",
    "    def forward(self, x, y):  # Accept two inputs\n",
    "        # Ensure output is wrapped in a tuple\n",
    "        return (F.gelu(x) + y,)  # Return as a tuple\n",
    "\n",
    "mod = MyModule()\n",
    "\n",
    "# Define example inputs as a tuple of two tensors\n",
    "example_input = (\n",
    "    torch.tensor([0.2, -0.5, 0.8, -1.2], requires_grad=True),\n",
    "    torch.tensor([1, -1, 1, -1], dtype=torch.float32)\n",
    ")\n",
    "\n",
    "def print_backend(gm: GraphModule, inputs):\n",
    "    print(\"Decomposed Graph:\")\n",
    "    print(gm.code)  # This will show decomposed ops like mul, erf, etc.\n",
    "    return gm.forward\n",
    "\n",
    "compiled = aot_module_simplified(\n",
    "    mod,\n",
    "    example_input,\n",
    "    fw_compiler=print_backend,\n",
    "    decompositions=core_aten_decompositions(),\n",
    ")\n",
    "\n",
    "# Call the compiled function with the two inputs\n",
    "compiled(*example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8752d1f2-c46c-49eb-b023-1c88a6d5df79",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MyModule.forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(gm\u001b[38;5;241m.\u001b[39mcode)  \u001b[38;5;66;03m# <- This will show decomposed ops like mul, erf, etc.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gm\u001b[38;5;241m.\u001b[39mforward\n\u001b[1;32m---> 21\u001b[0m compiled \u001b[38;5;241m=\u001b[39m aot_module_simplified(\n\u001b[0;32m     22\u001b[0m     mod,\n\u001b[0;32m     23\u001b[0m     example_input,\n\u001b[0;32m     24\u001b[0m     fw_compiler\u001b[38;5;241m=\u001b[39mprint_backend,\n\u001b[0;32m     25\u001b[0m     decompositions\u001b[38;5;241m=\u001b[39mcore_aten_decompositions(),\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     28\u001b[0m compiled(\u001b[38;5;241m*\u001b[39mexample_input)\n",
      "File \u001b[1;32mC:\\VAPPS\\anaconda3\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py:1071\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[1;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler, cudagraphs)\u001b[0m\n\u001b[0;32m   1067\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m AOTAutogradCache\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m   1068\u001b[0m         dispatch_and_compile, mod, fake_flat_args, aot_config, cudagraphs\n\u001b[0;32m   1069\u001b[0m     )\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1071\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m dispatch_and_compile()\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mGmWrapper):\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;66;03m# This function is called by the flatten_graph_inputs wrapper, which boxes\u001b[39;00m\n\u001b[0;32m   1075\u001b[0m     \u001b[38;5;66;03m# the inputs so that they can be freed before the end of this scope.\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m     \u001b[38;5;66;03m# For overhead reasons, this is not the default wrapper, see comment:\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/122535/files#r1560096481\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mboxed_forward\u001b[39m(runtime_args: List[Any]):\n",
      "File \u001b[1;32mC:\\VAPPS\\anaconda3\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py:1056\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.dispatch_and_compile\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1054\u001b[0m functional_call \u001b[38;5;241m=\u001b[39m create_functional_call(mod, params_spec, params_len)\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compiled_autograd\u001b[38;5;241m.\u001b[39mdisable():\n\u001b[1;32m-> 1056\u001b[0m     compiled_fn, _ \u001b[38;5;241m=\u001b[39m create_aot_dispatcher_function(\n\u001b[0;32m   1057\u001b[0m         functional_call,\n\u001b[0;32m   1058\u001b[0m         fake_flat_args,\n\u001b[0;32m   1059\u001b[0m         aot_config,\n\u001b[0;32m   1060\u001b[0m         fake_mode,\n\u001b[0;32m   1061\u001b[0m         shape_env,\n\u001b[0;32m   1062\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "File \u001b[1;32mC:\\VAPPS\\anaconda3\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py:522\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[1;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_aot_dispatcher_function\u001b[39m(\n\u001b[0;32m    515\u001b[0m     flat_fn,\n\u001b[0;32m    516\u001b[0m     fake_flat_args: FakifiedFlatArgs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m     shape_env: Optional[ShapeEnv],\n\u001b[0;32m    520\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Callable, ViewAndMutationMeta]:\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_aot_dispatcher_function\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 522\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _create_aot_dispatcher_function(\n\u001b[0;32m    523\u001b[0m             flat_fn, fake_flat_args, aot_config, fake_mode, shape_env\n\u001b[0;32m    524\u001b[0m         )\n",
      "File \u001b[1;32mC:\\VAPPS\\anaconda3\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py:623\u001b[0m, in \u001b[0;36m_create_aot_dispatcher_function\u001b[1;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[0;32m    621\u001b[0m     ctx \u001b[38;5;241m=\u001b[39m nullcontext()\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[1;32m--> 623\u001b[0m     fw_metadata \u001b[38;5;241m=\u001b[39m run_functionalized_fw_and_collect_metadata(\n\u001b[0;32m    624\u001b[0m         flat_fn,\n\u001b[0;32m    625\u001b[0m         static_input_indices\u001b[38;5;241m=\u001b[39maot_config\u001b[38;5;241m.\u001b[39mstatic_input_indices,\n\u001b[0;32m    626\u001b[0m         keep_input_mutations\u001b[38;5;241m=\u001b[39maot_config\u001b[38;5;241m.\u001b[39mkeep_inference_input_mutations,\n\u001b[0;32m    627\u001b[0m         is_train\u001b[38;5;241m=\u001b[39mneeds_autograd,\n\u001b[0;32m    628\u001b[0m         pre_dispatch\u001b[38;5;241m=\u001b[39maot_config\u001b[38;5;241m.\u001b[39mpre_dispatch,\n\u001b[0;32m    629\u001b[0m     )(\u001b[38;5;241m*\u001b[39m_dup_fake_script_obj(fake_flat_args))\n\u001b[0;32m    631\u001b[0m req_subclass_dispatch \u001b[38;5;241m=\u001b[39m requires_subclass_dispatch(\n\u001b[0;32m    632\u001b[0m     fake_flat_args, fw_metadata\n\u001b[0;32m    633\u001b[0m )\n\u001b[0;32m    635\u001b[0m output_and_mutation_safe \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    636\u001b[0m     x\u001b[38;5;241m.\u001b[39mrequires_grad\n\u001b[0;32m    637\u001b[0m     \u001b[38;5;66;03m# view-type operations preserve requires_grad even in no_grad.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    652\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m fw_metadata\u001b[38;5;241m.\u001b[39minput_info\n\u001b[0;32m    653\u001b[0m )\n",
      "File \u001b[1;32mC:\\VAPPS\\anaconda3\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\collect_metadata_analysis.py:173\u001b[0m, in \u001b[0;36mrun_functionalized_fw_and_collect_metadata.<locals>.inner\u001b[1;34m(*flat_args)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m disable_above, mode, suppress_pending:\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;66;03m# precondition: The passed in function already handles unflattening inputs + flattening outputs\u001b[39;00m\n\u001b[0;32m    172\u001b[0m     flat_f_args \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_map(_to_fun, flat_args)\n\u001b[1;32m--> 173\u001b[0m     flat_f_outs \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;241m*\u001b[39mflat_f_args)\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m# We didn't do any tracing, so we don't need to process the\u001b[39;00m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;66;03m# unbacked symbols, they will just disappear into the ether.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;66;03m# Also, prevent memoization from applying.\u001b[39;00m\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fake_mode:\n",
      "File \u001b[1;32mC:\\VAPPS\\anaconda3\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\traced_function_transforms.py:863\u001b[0m, in \u001b[0;36mcreate_functional_call.<locals>.functional_call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    859\u001b[0m                 out \u001b[38;5;241m=\u001b[39m PropagateUnbackedSymInts(mod)\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m    860\u001b[0m                     \u001b[38;5;241m*\u001b[39margs[params_len:], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    861\u001b[0m                 )\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 863\u001b[0m         out \u001b[38;5;241m=\u001b[39m mod(\u001b[38;5;241m*\u001b[39margs[params_len:], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraph output must be a (). This is so that we can avoid \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    868\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytree processing of the outputs. Please change the module to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    869\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave tuple outputs or use aot_module instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    870\u001b[0m     )\n",
      "File \u001b[1;32mC:\\VAPPS\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\VAPPS\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mTypeError\u001b[0m: MyModule.forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "from torch._functorch.aot_autograd import aot_module_simplified\n",
    "from torch._decomp import core_aten_decompositions\n",
    "\n",
    "from torch.fx import GraphModule\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class MyModule(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return F.gelu(x)\n",
    "\n",
    "mod = MyModule()\n",
    "#example_input = (torch.randn(4),)\n",
    "example_input = (torch.tensor([0.2, -0.5, 0.8, -1.2], requires_grad=True),torch.tensor([1, -1, 1, -1], dtype=torch.float32),)\n",
    "\n",
    "def print_backend(gm: GraphModule, inputs):\n",
    "    print(\"Decomposed Graph:\")\n",
    "    print(gm.code)  # <- This will show decomposed ops like mul, erf, etc.\n",
    "    return gm.forward\n",
    "\n",
    "compiled = aot_module_simplified(\n",
    "    mod,\n",
    "    example_input,\n",
    "    fw_compiler=print_backend,\n",
    "    decompositions=core_aten_decompositions(),\n",
    ")\n",
    "\n",
    "compiled(*example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181089f6-3fd1-4607-b5d2-d2e9ee563939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS THE SECOND ONE, IT WORKS BUT IT IS A BIT DIFFICULT TO MODIFY IT FOR OTHER OPERATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbd16a51-15ed-47c5-97da-bef649f359d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposed Graph:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2):\n",
      "    neg = torch.ops.aten.neg.default(primals_1)\n",
      "    mul = torch.ops.aten.mul.Tensor(neg, primals_2);  neg = None\n",
      "    exp = torch.ops.aten.exp.default(mul);  mul = None\n",
      "    log1p = torch.ops.aten.log1p.default(exp);  exp = None\n",
      "    mean = torch.ops.aten.mean.default(log1p);  log1p = None\n",
      "    return (mean, primals_1, primals_2)\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7598, grad_fn=<SoftMarginLossBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch._functorch.aot_autograd import aot_module_simplified\n",
    "from torch._decomp import core_aten_decompositions\n",
    "from torch.fx import GraphModule\n",
    "\n",
    "# Define a custom module that uses soft_margin_loss\n",
    "class MyModule(torch.nn.Module):\n",
    "    def forward(self, *inputs):\n",
    "        logits = inputs[0]\n",
    "        targets = inputs[1]\n",
    "        return (F.soft_margin_loss(logits, targets),)  # Use Soft Margin Loss in the forward pass\n",
    "\n",
    "# Function to compute the loss (involving soft margin loss)\n",
    "def compute_loss(logits, targets):\n",
    "    # Compute soft margin loss\n",
    "    loss = F.soft_margin_loss(logits, targets)\n",
    "    return loss\n",
    "\n",
    "# Example input tensors for logits and targets\n",
    "logits = torch.randn(4, requires_grad=True)\n",
    "targets = torch.tensor([1, -1, 1, -1], dtype=torch.float32)  # Targets must be -1 or 1\n",
    "\n",
    "# Define a custom backend to print the decomposed graph\n",
    "def print_backend(gm: GraphModule, inputs):\n",
    "    print(\"Decomposed Graph:\")\n",
    "    print(gm.code)  # This will print the decomposed graph code\n",
    "    return gm.forward  # Return the forward function for execution\n",
    "\n",
    "# Use aot_module_simplified to compile the module and apply decompositions\n",
    "compiled = aot_module_simplified(\n",
    "    MyModule(),\n",
    "    (logits, targets),\n",
    "    fw_compiler=print_backend,  # Custom backend to print the graph\n",
    "    decompositions=core_aten_decompositions(),  # Apply standard decompositions (including soft_margin_loss)\n",
    ")\n",
    "\n",
    "# Compute the loss by applying the forward pass (which triggers soft_margin_loss)\n",
    "compute_loss(logits, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ed9a3d-4df1-45b1-8266-165fe5c87c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS NOT WORKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8d81770-f321-49af-81b6-f9435ed62dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing Soft Margin Loss:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "aot_module_simplified() got multiple values for argument 'fw_compiler'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# Targets must be -1 or 1\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecomposing Soft Margin Loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m print_operator_decomposition(F\u001b[38;5;241m.\u001b[39msoft_margin_loss, logits, targets)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Example 2: ReLU\u001b[39;00m\n\u001b[0;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m4\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[6], line 24\u001b[0m, in \u001b[0;36mprint_operator_decomposition\u001b[1;34m(operator_fn, *inputs)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gm\u001b[38;5;241m.\u001b[39mforward  \u001b[38;5;66;03m# Return the forward function for execution\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Use aot_module_simplified to compile and trace the module\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m compiled \u001b[38;5;241m=\u001b[39m aot_module_simplified(\n\u001b[0;32m     25\u001b[0m     OperatorModule(),\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;241m*\u001b[39minputs,  \u001b[38;5;66;03m# Provide the inputs to the operator\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     fw_compiler\u001b[38;5;241m=\u001b[39mprint_backend,  \u001b[38;5;66;03m# Custom backend to print the graph\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     decompositions\u001b[38;5;241m=\u001b[39mcore_aten_decompositions(),  \u001b[38;5;66;03m# Apply standard decompositions\u001b[39;00m\n\u001b[0;32m     29\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: aot_module_simplified() got multiple values for argument 'fw_compiler'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch._functorch.aot_autograd import aot_module_simplified\n",
    "from torch._decomp import core_aten_decompositions\n",
    "from torch.fx import GraphModule\n",
    "\n",
    "# Function to print the decomposition of a given operator and its inputs\n",
    "def print_operator_decomposition(operator_fn, *inputs):\n",
    "    # Define a custom module that applies the operator\n",
    "    class OperatorModule(torch.nn.Module):\n",
    "        def forward(self, *inputs):\n",
    "            # Apply the given operator and return the result as a tuple\n",
    "            return (operator_fn(*inputs),)  # Return the result as a tuple\n",
    "    \n",
    "    # Use aot_module_simplified to compile the module and apply decompositions\n",
    "    def print_backend(gm: GraphModule, inputs):\n",
    "        print(\"Decomposed Graph:\")\n",
    "        # Print out the graph in a human-readable format\n",
    "        for node in gm.graph.nodes:\n",
    "            print(f\"{node.name} = {node.op}({', '.join(map(str, node.args))})\")\n",
    "        return gm.forward  # Return the forward function for execution\n",
    "\n",
    "    # Use aot_module_simplified to compile and trace the module\n",
    "    compiled = aot_module_simplified(\n",
    "        OperatorModule(),\n",
    "        *inputs,  # Provide the inputs to the operator\n",
    "        fw_compiler=print_backend,  # Custom backend to print the graph\n",
    "        decompositions=core_aten_decompositions(),  # Apply standard decompositions\n",
    "    )\n",
    "\n",
    "# Example usage: Allow dynamic input for the operator and its inputs\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: Soft Margin Loss\n",
    "    logits = torch.randn(4, requires_grad=True)\n",
    "    targets = torch.tensor([1, -1, 1, -1], dtype=torch.float32)  # Targets must be -1 or 1\n",
    "    print(\"Decomposing Soft Margin Loss:\")\n",
    "    print_operator_decomposition(F.soft_margin_loss, logits, targets)\n",
    "    \n",
    "    # Example 2: ReLU\n",
    "    x = torch.randn(4, requires_grad=True)\n",
    "    print(\"\\nDecomposing ReLU:\")\n",
    "    print_operator_decomposition(F.relu, x)\n",
    "    \n",
    "    # Example 3: Batch Normalization\n",
    "    x_bn = torch.randn(4, 4, requires_grad=True)\n",
    "    print(\"\\nDecomposing BatchNorm:\")\n",
    "    print_operator_decomposition(F.batch_norm, x_bn, running_mean=torch.zeros_like(x_bn), running_var=torch.ones_like(x_bn))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32daa8c9-ca9f-4c62-8a9c-b6aa6bca936c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
